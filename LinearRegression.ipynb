{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Linear Regression\n",
    "This project will work on how to predict the prices of homes based on the properties of the house. I will determine which house affected the final sale price and how effectively we can predict the sale price.Here's a brief description of the columns in the data:\n",
    "\n",
    "- Lot Area: Lot size in square feet.\n",
    "- Overall Qual: Rates the overall material and finish of the house.\n",
    "- Overall Cond: Rates the overall condition of the house.\n",
    "- Year Built: Original construction date.\n",
    "- Low Qual Fin SF: Low quality finished square feet (all floors).\n",
    "- Full Bath: Full bathrooms above grade.\n",
    "- Fireplaces: Number of fireplaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 82 columns):\n",
      "Order              1460 non-null int64\n",
      "PID                1460 non-null int64\n",
      "MS SubClass        1460 non-null int64\n",
      "MS Zoning          1460 non-null object\n",
      "Lot Frontage       1211 non-null float64\n",
      "Lot Area           1460 non-null int64\n",
      "Street             1460 non-null object\n",
      "Alley              109 non-null object\n",
      "Lot Shape          1460 non-null object\n",
      "Land Contour       1460 non-null object\n",
      "Utilities          1460 non-null object\n",
      "Lot Config         1460 non-null object\n",
      "Land Slope         1460 non-null object\n",
      "Neighborhood       1460 non-null object\n",
      "Condition 1        1460 non-null object\n",
      "Condition 2        1460 non-null object\n",
      "Bldg Type          1460 non-null object\n",
      "House Style        1460 non-null object\n",
      "Overall Qual       1460 non-null int64\n",
      "Overall Cond       1460 non-null int64\n",
      "Year Built         1460 non-null int64\n",
      "Year Remod/Add     1460 non-null int64\n",
      "Roof Style         1460 non-null object\n",
      "Roof Matl          1460 non-null object\n",
      "Exterior 1st       1460 non-null object\n",
      "Exterior 2nd       1460 non-null object\n",
      "Mas Vnr Type       1449 non-null object\n",
      "Mas Vnr Area       1449 non-null float64\n",
      "Exter Qual         1460 non-null object\n",
      "Exter Cond         1460 non-null object\n",
      "Foundation         1460 non-null object\n",
      "Bsmt Qual          1420 non-null object\n",
      "Bsmt Cond          1420 non-null object\n",
      "Bsmt Exposure      1419 non-null object\n",
      "BsmtFin Type 1     1420 non-null object\n",
      "BsmtFin SF 1       1459 non-null float64\n",
      "BsmtFin Type 2     1419 non-null object\n",
      "BsmtFin SF 2       1459 non-null float64\n",
      "Bsmt Unf SF        1459 non-null float64\n",
      "Total Bsmt SF      1459 non-null float64\n",
      "Heating            1460 non-null object\n",
      "Heating QC         1460 non-null object\n",
      "Central Air        1460 non-null object\n",
      "Electrical         1460 non-null object\n",
      "1st Flr SF         1460 non-null int64\n",
      "2nd Flr SF         1460 non-null int64\n",
      "Low Qual Fin SF    1460 non-null int64\n",
      "Gr Liv Area        1460 non-null int64\n",
      "Bsmt Full Bath     1459 non-null float64\n",
      "Bsmt Half Bath     1459 non-null float64\n",
      "Full Bath          1460 non-null int64\n",
      "Half Bath          1460 non-null int64\n",
      "Bedroom AbvGr      1460 non-null int64\n",
      "Kitchen AbvGr      1460 non-null int64\n",
      "Kitchen Qual       1460 non-null object\n",
      "TotRms AbvGrd      1460 non-null int64\n",
      "Functional         1460 non-null object\n",
      "Fireplaces         1460 non-null int64\n",
      "Fireplace Qu       743 non-null object\n",
      "Garage Type        1386 non-null object\n",
      "Garage Yr Blt      1385 non-null float64\n",
      "Garage Finish      1385 non-null object\n",
      "Garage Cars        1460 non-null float64\n",
      "Garage Area        1460 non-null float64\n",
      "Garage Qual        1385 non-null object\n",
      "Garage Cond        1385 non-null object\n",
      "Paved Drive        1460 non-null object\n",
      "Wood Deck SF       1460 non-null int64\n",
      "Open Porch SF      1460 non-null int64\n",
      "Enclosed Porch     1460 non-null int64\n",
      "3Ssn Porch         1460 non-null int64\n",
      "Screen Porch       1460 non-null int64\n",
      "Pool Area          1460 non-null int64\n",
      "Pool QC            1 non-null object\n",
      "Fence              297 non-null object\n",
      "Misc Feature       60 non-null object\n",
      "Misc Val           1460 non-null int64\n",
      "Mo Sold            1460 non-null int64\n",
      "Yr Sold            1460 non-null int64\n",
      "Sale Type          1460 non-null object\n",
      "Sale Condition     1460 non-null object\n",
      "SalePrice          1460 non-null int64\n",
      "dtypes: float64(11), int64(28), object(43)\n",
      "memory usage: 690.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:/Users/Jennifer/Documents/Python/Data/AmesHousing.txt\", delimiter = '\\t')\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "target = 'SalePrice'\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use linear regression to model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56034.3620014\n",
      "57088.2516126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[['Gr Liv Area']], train['SalePrice'])\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_predictions = lr.predict(train[['Gr Liv Area']])\n",
    "test_predictions = lr.predict(test[['Gr Liv Area']])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train['SalePrice'])\n",
    "test_mse = mean_squared_error(test_predictions, test['SalePrice'])\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Multiple Regression to model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56032.3980153\n",
      "57066.9077945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "cols = ['Overall Cond', 'Gr Liv Area']\n",
    "lr.fit(train[cols], train['SalePrice'])\n",
    "train_predictions = lr.predict(train[cols])\n",
    "test_predictions = lr.predict(test[cols])\n",
    "\n",
    "train_rmse_2 = np.sqrt(mean_squared_error(train_predictions, train['SalePrice']))\n",
    "test_rmse_2 = np.sqrt(mean_squared_error(test_predictions, test['SalePrice']))\n",
    "\n",
    "print(train_rmse_2)\n",
    "print(test_rmse_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling data types with missing values/non-numeric values\n",
    "In the machine learning workflow, once we've selected the model we want to use, selecting the appropriate features for that model is the next important step. In the following code snippets, I will explore how to use correlation between features and the target column, correlation between features, and variance of features to select features.\n",
    "\n",
    "I will specifically focus on selecting from feature columns that don't have any missing values or don't need to be transformed to be useful (e.g. columns like Year Built and Year Remod/Add). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order              0\n",
      "MS SubClass        0\n",
      "Lot Area           0\n",
      "Overall Qual       0\n",
      "Overall Cond       0\n",
      "1st Flr SF         0\n",
      "2nd Flr SF         0\n",
      "Low Qual Fin SF    0\n",
      "Gr Liv Area        0\n",
      "Full Bath          0\n",
      "Half Bath          0\n",
      "Bedroom AbvGr      0\n",
      "Kitchen AbvGr      0\n",
      "TotRms AbvGrd      0\n",
      "Fireplaces         0\n",
      "Garage Cars        0\n",
      "Garage Area        0\n",
      "Wood Deck SF       0\n",
      "Open Porch SF      0\n",
      "Enclosed Porch     0\n",
      "3Ssn Porch         0\n",
      "Screen Porch       0\n",
      "Pool Area          0\n",
      "Misc Val           0\n",
      "SalePrice          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numerical_train = train.select_dtypes(include=['int64', 'float'])\n",
    "numerical_train = numerical_train.drop(['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold'], axis=1)\n",
    "null_series = numerical_train.isnull().sum()\n",
    "full_cols_series = null_series[null_series == 0]\n",
    "print(full_cols_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlating feature columns with Target Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misc Val           0.009903\n",
      "3Ssn Porch         0.038699\n",
      "Low Qual Fin SF    0.060352\n",
      "Order              0.068181\n",
      "MS SubClass        0.088504\n",
      "Overall Cond       0.099395\n",
      "Screen Porch       0.100121\n",
      "Bedroom AbvGr      0.106941\n",
      "Kitchen AbvGr      0.130843\n",
      "Pool Area          0.145474\n",
      "Enclosed Porch     0.165873\n",
      "2nd Flr SF         0.202352\n",
      "Half Bath          0.272870\n",
      "Lot Area           0.274730\n",
      "Wood Deck SF       0.319104\n",
      "Open Porch SF      0.344383\n",
      "TotRms AbvGrd      0.483701\n",
      "Fireplaces         0.485683\n",
      "Full Bath          0.518194\n",
      "1st Flr SF         0.657119\n",
      "Garage Area        0.662397\n",
      "Garage Cars        0.663485\n",
      "Gr Liv Area        0.698990\n",
      "Overall Qual       0.804562\n",
      "SalePrice          1.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_subset = train[full_cols_series.index]\n",
    "corrmat = train_subset.corr()\n",
    "sorted_corrs = corrmat['SalePrice'].abs().sort_values()\n",
    "print(sorted_corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix Heatmap\n",
    "We now have a decent list of candidate features to use in our model, sorted by how strongly they're correlated with the SalePrice column. For now, I will keep only the features that have a correlation of 0.3 or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\n",
    "\n",
    "The next thing we need to look for is for potential collinearity between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we need to choose just one or predictive accuracy can suffer.\n",
    "\n",
    "While we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a correlation matrix heatmap using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we're looking for outlier values in the heatmap, this visual representation is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x6a7fe70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,6))\n",
    "strong_corrs = sorted_corrs[sorted_corrs > 0.3]\n",
    "corrmat = train_subset[strong_corrs.index].corr()\n",
    "sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the model\n",
    "Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n",
    "\n",
    "- Gr Liv Area and TotRms AbvGrd\n",
    "- Garage Area and Garage Cars\n",
    "\n",
    "We will only use one of these pairs and remove any columns with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34173.9762919\n",
      "41032.0261202\n"
     ]
    }
   ],
   "source": [
    "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "features = final_corr_cols.drop(['SalePrice']).index\n",
    "target = 'SalePrice'\n",
    "clean_test = test[final_corr_cols.index].dropna()\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[features], train['SalePrice'])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train[target])\n",
    "test_mse = mean_squared_error(test_predictions, clean_test[target])\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing low variance features\n",
    "The last technique I will explore is removing features with low variance. When the values in a feature column have low variance, they don't meaningfully contribute to the model's predictive capability. On the extreme end, let's imagine a column with a variance of 0. This would mean that all of the values in that column were exactly the same. This means that the column isn't informative and isn't going to help the model make better predictions.\n",
    "\n",
    "To make apples to apples comparisions between columns, we need to standardize all of the columns to vary between 0 and 1. Then, we can set a cutoff value for variance and remove features that have less than that variance amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Porch SF    0.013938\n",
      "Gr Liv Area      0.018014\n",
      "Full Bath        0.018621\n",
      "1st Flr SF       0.019182\n",
      "Overall Qual     0.019842\n",
      "Garage Area      0.020347\n",
      "Wood Deck SF     0.033064\n",
      "Fireplaces       0.046589\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "unit_train = train[features]/(train[features].max())\n",
    "sorted_vars = unit_train.var().sort_values()\n",
    "print(sorted_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "Let's set a cutoff variance of 0.015, remove the Open Porch SF feature, and train and test a model using the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34372.6967078\n",
      "40591.4270244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = features.drop(['Open Porch SF'])\n",
    "\n",
    "clean_test = test[final_corr_cols.index].dropna()\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[features], train['SalePrice'])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train[target])\n",
    "test_mse = mean_squared_error(test_predictions, clean_test[target])\n",
    "\n",
    "train_rmse_2 = np.sqrt(train_mse)\n",
    "test_rmse_2 = np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse_2)\n",
    "print(test_rmse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation\n",
    "To understand how linear regression works, I have stuck to using features from the training dataset that contained no missing values and were already in a convenient numeric representation. In this mission, we'll explore how to transform some of the the remaining features so we can use them in our model. Broadly, the process of processing and creating new features is known as feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "train_null_counts = train.isnull().sum()\n",
    "df_no_mv = train[train_null_counts[train_null_counts==0].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features\n",
    "You'll notice that some of the columns in the data frame df_no_mv contain string values. To use these features in our model, we need to transform them into numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS Zoning: 6\n",
      "Street: 2\n",
      "Lot Shape: 4\n",
      "Land Contour: 4\n",
      "Utilities: 3\n",
      "Lot Config: 5\n",
      "Land Slope: 3\n",
      "Neighborhood: 26\n",
      "Condition 1: 9\n",
      "Condition 2: 6\n",
      "Bldg Type: 5\n",
      "House Style: 8\n",
      "Roof Style: 6\n",
      "Roof Matl: 5\n",
      "Exterior 1st: 14\n",
      "Exterior 2nd: 16\n",
      "Exter Qual: 4\n",
      "Exter Cond:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jennifer\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n",
      "Foundation: 6\n",
      "Heating: 6\n",
      "Heating QC: 4\n",
      "Central Air: 2\n",
      "Electrical: 4\n",
      "Kitchen Qual: 5\n",
      "Functional: 7\n",
      "Paved Drive: 3\n",
      "Sale Type: 9\n",
      "Sale Condition: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1457\n",
       "2       2\n",
       "1       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    print(col+\":\", len(train[col].unique()))\n",
    "for col in text_cols:\n",
    "    train[col] = train[col].astype('category')\n",
    "train['Utilities'].cat.codes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Coding\n",
    "When we convert a column to the categorical data type, pandas assigns a number from 0 to n-1 (where n is the number of unique values in a column) for each value. The drawback with this approach is that one of the assumptions of linear regression is violated here. Linear regression operates under the assumption that the features are linearly correlated with the target column. For a categorical feature, however, there's no actual numerical meaning to the categorical codes that pandas assigned for that colum. An increase in the Utilities column from 1 to 2 has no correlation value with the target column, and the categorical codes are instead used for uniqueness and exclusivity (the category associated with 0 is different than the one associated with 1).\n",
    "\n",
    "The common solution is to use a technique called dummy coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_cols = pd.DataFrame()\n",
    "for col in text_cols:\n",
    "    col_dummies = pd.get_dummies(train[col])\n",
    "    train = pd.concat([train, col_dummies], axis=1)\n",
    "    del train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['years_until_remod'] = train['Year Remod/Add'] - train['Year Built']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "Now I will focus on handling columns with missing values. When values are missing in a column, there are two main approaches we can take:\n",
    "\n",
    "- Remove rows containing missing values for specific columns\n",
    "Pro: Rows containing missing values are removed, leaving only clean data for modeling\n",
    "Con: Entire observations from the training set are removed, which can reduce overall prediction accuracy\n",
    "- Impute (or replace) missing values using a descriptive statistic from the column\n",
    "Pro: Missing values are replaced with potentially similar estimates, preserving the rest of the observation in the model.\n",
    "Con: Depending on the approach, we may be adding noisy data for the model to learn\n",
    "\n",
    "Given that we only have 1460 training examples (with ~80 potentially useful features), we don't want to remove any of these rows from the dataset. Let's instead focus on imputation techniques.\n",
    "\n",
    "We'll focus on columns that contain at least 1 missing value but less than 365 missing values (or 25% of the number of rows in the training set). There's no strict threshold, and many people instead use a 50% cutoff (if half the values in a column are missing, it's automatically dropped). Having some domain knowledge can help with determining an acceptable cutoff value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lot Frontage      249\n",
      "Mas Vnr Type       11\n",
      "Mas Vnr Area       11\n",
      "Bsmt Qual          40\n",
      "Bsmt Cond          40\n",
      "Bsmt Exposure      41\n",
      "BsmtFin Type 1     40\n",
      "BsmtFin SF 1        1\n",
      "BsmtFin Type 2     41\n",
      "BsmtFin SF 2        1\n",
      "Bsmt Unf SF         1\n",
      "Total Bsmt SF       1\n",
      "Bsmt Full Bath      1\n",
      "Bsmt Half Bath      1\n",
      "Garage Type        74\n",
      "Garage Yr Blt      75\n",
      "Garage Finish      75\n",
      "Garage Qual        75\n",
      "Garage Cond        75\n",
      "dtype: int64\n",
      "Lot Frontage      float64\n",
      "Mas Vnr Type       object\n",
      "Mas Vnr Area      float64\n",
      "Bsmt Qual          object\n",
      "Bsmt Cond          object\n",
      "Bsmt Exposure      object\n",
      "BsmtFin Type 1     object\n",
      "BsmtFin SF 1      float64\n",
      "BsmtFin Type 2     object\n",
      "BsmtFin SF 2      float64\n",
      "Bsmt Unf SF       float64\n",
      "Total Bsmt SF     float64\n",
      "Bsmt Full Bath    float64\n",
      "Bsmt Half Bath    float64\n",
      "Garage Type        object\n",
      "Garage Yr Blt     float64\n",
      "Garage Finish      object\n",
      "Garage Qual        object\n",
      "Garage Cond        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_missing_values = train[train_null_counts[(train_null_counts>0) & (train_null_counts<584)].index]\n",
    "\n",
    "print(df_missing_values.isnull().sum())\n",
    "print(df_missing_values.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing values\n",
    "It looks like about half of the columns in df_missing_values are string columns (object data type), while about half are float64 columns. For numerical columns with missing values, a common strategy is to compute the mean, median, or mode of each column and replace all missing values in that column with that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lot Frontage      0\n",
      "Mas Vnr Area      0\n",
      "BsmtFin SF 1      0\n",
      "BsmtFin SF 2      0\n",
      "Bsmt Unf SF       0\n",
      "Total Bsmt SF     0\n",
      "Bsmt Full Bath    0\n",
      "Bsmt Half Bath    0\n",
      "Garage Yr Blt     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
    "float_cols = float_cols.fillna(df_missing_values.mean())\n",
    "print(float_cols.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This concludes the project where we underook applying a linear regression model and handled missing values and non-numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
